import json
import logging
from fastapi import FastAPI, File, UploadFile, BackgroundTasks
from fastapi.responses import JSONResponse
from mangum import Mangum
import os
import uuid
import time
from pathlib import Path
from dotenv import load_dotenv
import boto3
import cv2
import numpy as np
import torch
from insightface.app import FaceAnalysis
from elasticsearch import Elasticsearch
from typing import List

logger = logging.getLogger()
logger.setLevel(logging.INFO)

# Only use StreamHandler for Lambda
if not logger.handlers:
    handler = logging.StreamHandler()
    handler.setFormatter(logging.Formatter("%(asctime)s - %(levelname)s - %(message)s"))
    logger.addHandler(handler)

# â”€â”€â”€ Load AWS credentials & config from .env â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
load_dotenv()

AWS_REGION = os.getenv("AWS_REGION", "ap-south-1")
S3_BUCKET = os.getenv("S3_BUCKET_NAME", "divinepic-test")
S3_UPLOAD_PATH = os.getenv("S3_UPLOAD_PATH", "upload_with_embed")

# â”€â”€â”€ Local folder to store uploaded files temporarily â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
TEMP_UPLOAD_DIR = "/tmp/uploads"

# â”€â”€â”€ Elasticsearch setup â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
ES_HOSTS = [
    os.getenv("ES_HOSTS1", "http://3.6.116.114:9200")  # Add second host if needed
]

INDEX_NAME = os.getenv("INDEX_NAME", "face_embeddings")

# â”€â”€â”€ Initialize AWS S3 client with error handling â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
try:
    s3_client = boto3.client("s3", region_name=AWS_REGION)
    # Test S3 connection
    s3_client.head_bucket(Bucket=S3_BUCKET)
    logger.info(f"âœ… S3 client initialized successfully for bucket: {S3_BUCKET}")
except Exception as e:
    logger.error(f"âŒ Failed to initialize S3 client: {e}")
    s3_client = None

# â”€â”€â”€ Initialize Elasticsearch clients (will connect when first used) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
es_clients = []

app = FastAPI(root_path="/prod")

# â”€â”€â”€ Ensure temporary upload directory exists â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
if not os.path.isdir(TEMP_UPLOAD_DIR):
    os.makedirs(TEMP_UPLOAD_DIR)

# â”€â”€â”€ Initialize ES clients lazily â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def get_es_clients():
    """Get or initialize Elasticsearch clients (lazy loading)."""
    global es_clients
    if not es_clients:
        logger.info("ğŸ”Œ Initializing Elasticsearch clients...")
        for host in ES_HOSTS:
            try:
                client = Elasticsearch([host], verify_certs=False)
                # Test connection
                client.info()
                es_clients.append((client, host))
                logger.info(f"âœ… Connected to Elasticsearch at {host}")
            except Exception as e:
                logger.error(f"âŒ Failed to connect to Elasticsearch at {host}: {e}")
    return es_clients

# â”€â”€â”€ Create Elasticsearch index if needed â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def create_index_if_not_exists():
    """Create the index (if it doesn't exist) on all ES clusters with the same mapping."""
    mapping_body = {
        "settings": {
            "number_of_shards": 1,
            "number_of_replicas": 0
        },
        "mappings": {
            "properties": {
                "image_name": { "type": "keyword" },
                "embeds": {
                    "type": "dense_vector",
                    "dims": 512,
                    "index": True,
                    "similarity": "cosine"
                },
                "box": {
                    "type": "dense_vector",
                    "dims": 4
                }
            }
        }
    }

    clients = get_es_clients()
    for client, host in clients:
        try:
            if client.indices.exists(index=INDEX_NAME):
                logger.info(f"â„¹ï¸  Index '{INDEX_NAME}' already exists on {host}")
            else:
                logger.info(f"ğŸš€ Creating index '{INDEX_NAME}' on {host}")
                client.indices.create(index=INDEX_NAME, body=mapping_body)
                logger.info(f"âœ… Index '{INDEX_NAME}' created successfully on {host}")
        except Exception as e:
            logger.error(f"âŒ Failed to create index on {host}: {e}")

# â”€â”€â”€ Helper function to extract date from filename (if needed) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def extract_date_from_filename(filename: str) -> str:
    """Extract date from filename and return formatted date string (DD_MON_YYYY)."""
    try:
        parts = filename.split("_")
        ts_ms = int(parts[1])
        dt = time.strftime("%d_%b_%Y", time.gmtime(ts_ms / 1000.0))
        return dt.upper()
    except (IndexError, ValueError):
        return time.strftime("%d_%b_%Y", time.gmtime()).upper()

# â”€â”€â”€ Upload image to S3 â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def upload_image_to_s3(local_path: str) -> str:
    """Upload image to S3 and return public URL."""
    if s3_client is None:
        raise RuntimeError("S3 client not initialized. Check AWS credentials and bucket access.")
        
    filename = Path(local_path).name
    file_ext = Path(filename).suffix.lower()
    
    if file_ext not in [".jpg", ".jpeg", ".png", ".bmp", ".tiff", ".webp"]:
        raise ValueError(f"Unsupported file extension: {file_ext}")

    unique_key = f"{uuid.uuid4().hex}_{filename}"
    s3_key = f"{S3_UPLOAD_PATH.rstrip('/')}/{unique_key}"

    with open(local_path, "rb") as f:
        data = f.read()

    try:
        s3_client.put_object(
            Bucket=S3_BUCKET,
            Key=s3_key,
            Body=data,
            ContentType=f"image/{file_ext.lstrip('.')}"
        )
    except Exception as e:
        logger.error(f"âš ï¸ Failed to upload '{local_path}' to S3: {e}")
        raise

    public_url = f"https://{S3_BUCKET}.s3.{AWS_REGION}.amazonaws.com/{s3_key}"
    return public_url

# â”€â”€â”€ Initialize face detection model (global to avoid reloading) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
face_app = None

def get_face_model():
    """Get or initialize the face detection model (singleton pattern)."""
    global face_app
    if face_app is None:
        DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
        logger.info(f"Using device: {DEVICE}")

        face_app = FaceAnalysis(
            name="antelopev2",
            root="/app/models",
            providers=["CPUExecutionProvider"] if DEVICE == "cpu" else ["CUDAExecutionProvider"]
        )
        face_app.prepare(ctx_id=0 if DEVICE == "cuda" else -1, det_thresh=0.35, det_size=(640, 640))

        logger.info("âœ… antelopev2 model loaded successfully")
    return face_app

# â”€â”€â”€ Process images in the background (generate embeddings and index to ES) â”€â”€â”€â”€
def process_images_and_generate_embeddings(image_paths: List[str]):
    """Process images to generate face embeddings and index them into Elasticsearch."""
    face_model = get_face_model()  # Get the model instance
    
    # Ensure index exists
    create_index_if_not_exists()
    
    processed_count = 0
    total_faces = 0
    
    for image_path in image_paths:
        try:
            # Upload image to S3
            s3_url = upload_image_to_s3(image_path)
            
            # Read image and run face detection
            img_bgr = cv2.imread(image_path)
            if img_bgr is None:
                logger.warning(f"âš ï¸ Could not read image '{image_path}'. Skipping...")
                continue
                
            img_rgb = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2RGB)
            faces = face_model.get(img_rgb)
            num_faces = len(faces)
            
            if not faces:
                logger.info(f"â„¹ï¸  No faces detected in '{image_path}'. Still uploaded to S3 â†’ URL: {s3_url}")
                processed_count += 1
                continue
            
            # Log faces and index to Elasticsearch
            for idx, face in enumerate(faces):
                emb_vec = face.normed_embedding
                box_coords = face.bbox.tolist()

                face_id = f"{Path(image_path).stem}_face_{idx+1}_{uuid.uuid4().hex[:8]}"
                doc = {
                    "image_name": s3_url,
                    "embeds": emb_vec.tolist(),
                    "box": box_coords
                }

                # Index the face into all Elasticsearch clusters
                clients = get_es_clients()
                for client, host in clients:
                    try:
                        client.index(index=INDEX_NAME, id=face_id, document=doc)
                        logger.info(f"âœ… Indexed face {idx+1} from '{Path(image_path).name}' into ES ({host})")
                    except Exception as e:
                        logger.error(f"âŒ Failed to index face {idx+1} from '{Path(image_path).name}' into ES ({host}): {e}")
            
            total_faces += num_faces
            processed_count += 1
            logger.info(f"âœ… Processed '{Path(image_path).name}' â†’ faces: {num_faces}, S3 URL: {s3_url}")
            
        except Exception as e:
            logger.error(f"âš ï¸ Error processing '{image_path}': {e}")
        finally:
            # Clean up temporary file
            try:
                os.remove(image_path)
            except:
                pass
    
    logger.info(f"ğŸ Bulk processing complete: {processed_count} images processed, {total_faces} faces indexed")

# â”€â”€â”€ Existing endpoints â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

@app.get("/")
async def root() -> dict:
    """
    **Dummy endpoint that returns 'hello world' example.**

    ```
    Returns:
        dict: 'hello world' message.
    ```
    """
    return {"message": "Hello World"}


@app.get("/question")
async def get_answer(question: str, context: str) -> dict:
    """
    **Endpoint implementing the question-answering logic.**

    ```
    Args:
        question (str): Question to be answered. Answer should be included in 'context'.
        context (str): Context containing information necessary to answer 'question'.
    Returns:
        dict: Dictionary containing the answer to the question along with some metadata.
    ```
    """
    from custom_lambda_utils.scripts.inference import question_answerer

    return question_answerer(question=question, context=context)

# â”€â”€â”€ New face detection endpoints â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

@app.post("/upload-images/")
async def upload_images(background_tasks: BackgroundTasks, files: List[UploadFile] = File(...)):
    """Endpoint to upload multiple images and process them in the background."""
    logger.info(f"â³ Starting bulk image upload for {len(files)} files...")

    if not files:
        return JSONResponse(
            status_code=400,
            content={"error": "No files provided"}
        )

    # Validate file extensions
    valid_exts = (".jpg", ".jpeg", ".png", ".bmp", ".tiff", ".webp")
    invalid_files = []
    
    # Temporarily store the images in the local directory
    image_paths = []
    for uploaded_file in files:
        file_ext = Path(uploaded_file.filename).suffix.lower()
        if file_ext not in valid_exts:
            invalid_files.append(uploaded_file.filename)
            continue
            
        image_path = os.path.join(TEMP_UPLOAD_DIR, uploaded_file.filename)
        try:
            with open(image_path, "wb") as buffer:
                content = await uploaded_file.read()
                buffer.write(content)
            image_paths.append(image_path)
        except Exception as e:
            logger.error(f"Failed to save uploaded file {uploaded_file.filename}: {e}")
            invalid_files.append(uploaded_file.filename)
    
    if invalid_files:
        logger.warning(f"âš ï¸ Skipped {len(invalid_files)} invalid files: {invalid_files}")
    
    if not image_paths:
        return JSONResponse(
            status_code=400,
            content={"error": "No valid image files provided", "invalid_files": invalid_files}
        )
    
    logger.info(f"âœ… {len(image_paths)} valid images uploaded successfully to backend.")
    
    # Background task to process the images and generate embeddings
    background_tasks.add_task(process_images_and_generate_embeddings, image_paths)

    response_data = {
        "message": f"Images uploaded successfully! Processing {len(image_paths)} images for face detection and embedding generation.",
        "valid_files": len(image_paths),
        "total_files": len(files)
    }
    
    if invalid_files:
        response_data["invalid_files"] = invalid_files
        response_data["skipped_files"] = len(invalid_files)

    return JSONResponse(status_code=200, content=response_data)

# â”€â”€â”€ Health check endpoint â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
@app.get("/health")
async def health_check():
    """Simple health check endpoint."""
    return {"status": "healthy", "service": "face-detection-api"}

# â”€â”€â”€ Elasticsearch connection test endpoint â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
@app.get("/test-es")
async def test_elasticsearch():
    """Test Elasticsearch connections."""
    results = []
    clients = get_es_clients()
    for client, host in clients:
        try:
            info = client.info()
            results.append({"host": host, "status": "connected", "version": info.get("version", {}).get("number", "unknown")})
        except Exception as e:
            results.append({"host": host, "status": "error", "error": str(e)})
    
    return {"elasticsearch_connections": results}


def lambda_handler(event, context):
    logger.info(json.dumps(event))

    asgi_handler = Mangum(app)
    response = asgi_handler(
        event, context
    )  # Call the instance with the event arguments

    logger.info(json.dumps(response))
    return response
